from selenium import webdriver
import requests, bs4, re
from urllib.parse import urljoin
import pandas as pd

start_url = ‘http://www.mainpage.com/’
login_url = ‘http://login.com’

username = "Username"
password = "Password"

driver = webdriver.Ie(executable_path = “C:\”) #Ie (Internet Explorer) driver if needed can be downloaded from https://www.seleniumhq.org/download/
driver.get(login_url)
username_field = driver.find_element_by_id("username") #will need to find field id for specific login page
password_field = driver.find_element_by_id("password") #will need to find field id for specific login page
login_button = driver.find_element_by_id("login-button") #will need to find field id for specific login page

username_field.send_keys(username)
password_field.send_keys(password)
login_button.click()

def make_soup(url):
    r = requests.get(url)
    soup = bs4.BeautifulSoup(r.text, 'lxml')
    return soup

def get_links(url):
    soup = make_soup(url)
    a_tags = soup.find_all('a', href=re.compile(r"^/FINDTHISFIELDINURL/"))
    links = [urljoin(start_url, a['href'])for a in a_tags]  # convert relative url to absolute url
    return links

def get_tds(link):
     list_data = []
    soup = make_soup(link)
    tds = soup.find_all('td',  class_="enter your class here if any”)
    if not tds:
        print(link, 'did not find table or non match’)
    else:
        for td in tds:
            print(td.text)
            list_data.append(td) 

if __name__ == '__main__':
    links = get_links(start_url)
    for link in links:
        get_tds(link)
df_list_data = pd.DataFrame(list_data)
df_list_data.to_csv(“path”)

